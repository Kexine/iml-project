{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pullelys/iml-project/blob/main/task_2/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1PiFkfsn90M",
        "outputId": "77022b93-e653-4c94-fcb7-fb304df0dc18"
      },
      "source": [
        "!pip install tensorflow==2.1.0\n",
        "!pip install keras==2.3.1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/d4/c0cd1057b331bc38b65478302114194bd8e1b9c2bbc06e300935c0e93d90/tensorflow-2.1.0-cp36-cp36m-manylinux2010_x86_64.whl (421.8MB)\n",
            "\u001b[K     |████████████████████████████████| 421.8MB 21kB/s \n",
            "\u001b[?25hCollecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (0.8.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.1.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.18.5)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (3.12.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (0.2.0)\n",
            "Collecting keras-applications>=1.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.4MB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator<2.2.0,>=2.1.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/90/b77c328a1304437ab1310b463e533fa7689f4bfc41549593056d812fab8e/tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 50.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.15.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.34.0)\n",
            "Collecting tensorboard<2.2.0,>=2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d9/41/bbf49b61370e4f4d245d4c6051dfb6db80cec672605c91b1652ac8cc3d38/tensorboard-2.1.1-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.9MB 43.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.12.1)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (0.36.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (0.10.0)\n",
            "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.4.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (3.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorflow==2.1.0) (50.3.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==2.1.0) (2.10.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.17.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.3.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.4.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (4.1.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.1.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.24.3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.4.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.1.0)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7542 sha256=88965846426caf423e03f05eef51affd58c5598b6eebc73bdd06bb63e60ada47\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: tensorflow-probability 0.11.0 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: gast, keras-applications, tensorflow-estimator, tensorboard, tensorflow\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorflow-estimator 2.3.0\n",
            "    Uninstalling tensorflow-estimator-2.3.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.3.0\n",
            "  Found existing installation: tensorboard 2.3.0\n",
            "    Uninstalling tensorboard-2.3.0:\n",
            "      Successfully uninstalled tensorboard-2.3.0\n",
            "  Found existing installation: tensorflow 2.3.0\n",
            "    Uninstalling tensorflow-2.3.0:\n",
            "      Successfully uninstalled tensorflow-2.3.0\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-2.1.1 tensorflow-2.1.0 tensorflow-estimator-2.1.0\n",
            "Collecting keras==2.3.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/fd/6bfe87920d7f4fd475acd28500a42482b6b84479832bdc0fe9e589a60ceb/Keras-2.3.1-py2.py3-none-any.whl (377kB)\n",
            "\u001b[K     |████████████████████████████████| 378kB 9.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.15.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (2.10.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (3.13)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.1.2)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.18.5)\n",
            "Installing collected packages: keras\n",
            "  Found existing installation: Keras 2.4.3\n",
            "    Uninstalling Keras-2.4.3:\n",
            "      Successfully uninstalled Keras-2.4.3\n",
            "Successfully installed keras-2.3.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEZWvVHpZEQI",
        "outputId": "9e176912-3cd2-4f67-b3f3-c9039afb4128"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "import time\n",
        "import keras\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Activation, Dense, Dropout\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from keras.constraints import maxnorm\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.ensemble import RandomForestRegressor"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpK_n1XGo7C_",
        "outputId": "d2ec1186-4ea9-43f4-8f32-fe1f925abc68"
      },
      "source": [
        "# copy necessary files from Google Cloud Storage (GCS) to local disk\n",
        "bucket_name = 'colab-bucket-86f9472c-3ef9-11eb-a0d2-0242ac1c0002'\n",
        "files = ['train_labels.csv', 'train_features.csv', 'test_features.csv']\n",
        "for file in files:\n",
        "  !gsutil cp gs://{bucket_name}/{file} ."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://colab-bucket-86f9472c-3ef9-11eb-a0d2-0242ac1c0002/train_labels.csv...\n",
            "/ [1 files][891.7 KiB/891.7 KiB]                                                \n",
            "Operation completed over 1 objects/891.7 KiB.                                    \n",
            "Copying gs://colab-bucket-86f9472c-3ef9-11eb-a0d2-0242ac1c0002/train_features.csv...\n",
            "- [1 files][ 34.2 MiB/ 34.2 MiB]                                                \n",
            "Operation completed over 1 objects/34.2 MiB.                                     \n",
            "Copying gs://colab-bucket-86f9472c-3ef9-11eb-a0d2-0242ac1c0002/test_features.csv...\n",
            "/ [1 files][ 22.8 MiB/ 22.8 MiB]                                                \n",
            "Operation completed over 1 objects/22.8 MiB.                                     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JppC03CZEQV"
      },
      "source": [
        "# define main file name\n",
        "main_filename = 'main'\n",
        "\n",
        "# read and sort data\n",
        "X_train = pd.read_csv('train_features.csv').sort_values(by=['pid', 'Time'])\n",
        "y_train = pd.read_csv('train_labels.csv').sort_values(by=['pid'])\n",
        "X_test = pd.read_csv('test_features.csv').sort_values(by=['pid', 'Time'])\n",
        "\n",
        "# define column names as specified in the correct submission format\n",
        "# partition them into the corresponding subtasks\n",
        "subtask_1 = ['LABEL_BaseExcess', 'LABEL_Fibrinogen', 'LABEL_AST', 'LABEL_Alkalinephos', \n",
        "             'LABEL_Bilirubin_total', 'LABEL_Lactate', 'LABEL_TroponinI', 'LABEL_SaO2', \n",
        "             'LABEL_Bilirubin_direct', 'LABEL_EtCO2']\n",
        "subtask_2 = ['LABEL_Sepsis']\n",
        "subtask_3 = ['LABEL_RRate', 'LABEL_ABPm', 'LABEL_SpO2', 'LABEL_Heartrate']\n",
        "\n",
        "# initialize df_predictions with unique 'pid' column from X_test\n",
        "df_predictions = pd.DataFrame(X_test['pid'].unique(), columns=['pid'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09SXF-e_eypw",
        "outputId": "4a3104b2-1823-4ea0-dd45-becfa5d36323"
      },
      "source": [
        "#### CLASSIFICATION: SUBTASK 1 & 2\n",
        "\n",
        "# prepare features, disregard 'pid' and 'Time' columns\n",
        "relevant_features = X_train.columns[2:]\n",
        "\n",
        "# feature engineering\n",
        "# For each ‘pid’ and relevant column, we compute the following features: \n",
        "# mean, min, max, difference between min and max, first available (i.e. not nan) observation, \n",
        "# last available observation, difference between first and last, \n",
        "# the number of missing values over all 12 observations. \n",
        "# Whenever there is no observation per ‘pid’ and relevant column, \n",
        "# we impute the value with the mean of that column over the entire dataset.\n",
        "def clf_features(X):\n",
        "    X_mean = X[relevant_features].mean()\n",
        "\n",
        "    X_pid_mean = X.groupby(['pid'], as_index=False)[relevant_features].mean().drop(['pid'], axis=1)\n",
        "    X_pid_mean.fillna({col:X_mean[col] for col in X_pid_mean.columns}, inplace=True)\n",
        "\n",
        "    X_pid_min = X.groupby(['pid'], as_index=False)[relevant_features].min().drop(['pid', 'Age'], axis=1)\n",
        "    X_pid_min.fillna({col:X_mean[col] for col in X_pid_min.columns}, inplace=True)\n",
        "\n",
        "    X_pid_max = X.groupby(['pid'], as_index=False)[relevant_features].max().drop(['pid', 'Age'], axis=1)\n",
        "    X_pid_max.fillna({col:X_mean[col] for col in X_pid_max.columns}, inplace=True)\n",
        "\n",
        "    X_pid_diff_0 = X_pid_max-X_pid_min\n",
        "\n",
        "    X_pid_first = X.groupby(['pid'], as_index=False)[relevant_features].first().drop(['pid', 'Age'], axis=1)\n",
        "    X_pid_first.fillna({col:X_mean[col] for col in X_pid_first.columns}, inplace=True)\n",
        "\n",
        "    X_pid_last = X.groupby(['pid'], as_index=False)[relevant_features].last().drop(['pid', 'Age'], axis=1)\n",
        "    X_pid_last.fillna({col:X_mean[col] for col in X_pid_last.columns}, inplace=True)\n",
        "\n",
        "    X_pid_diff_1 = X_pid_last-X_pid_first\n",
        "\n",
        "    X_pid_missing = X.groupby(['pid'], as_index=False)[relevant_features].count().drop(['pid', 'Age'], axis=1)\n",
        "\n",
        "    X_final = pd.concat([X_pid_mean, X_pid_min, X_pid_max, X_pid_diff_0, X_pid_first, \n",
        "                        X_pid_last, X_pid_diff_1, X_pid_missing], axis=1).values\n",
        "    return X_final\n",
        "\n",
        "X_train_clf, X_test_clf = [clf_features(X) for X in [X_train, X_test]]\n",
        "\n",
        "# define function to create an ANN classifier model\n",
        "def create_model(input_dim=1, output_dim=1, n1_units=100, n2_units=100, n3_units=100, \n",
        "                 activation='relu', optimizer='Adam', visible_drop_rate=0.2, hidden_drop_rate=0.5, \n",
        "                 init_mode='glorot_uniform', maxnorm_value=3):\n",
        "    # input layer\n",
        "    visible = Input(shape=(input_dim,))\n",
        "    drop0 = Dropout(visible_drop_rate)(visible)\n",
        "    # hidden layer 1\n",
        "    hidden1 = Dense(n1_units, kernel_initializer=init_mode, kernel_constraint=maxnorm(maxnorm_value))(drop0)\n",
        "    batch1 = BatchNormalization()(hidden1)\n",
        "    act1 = Activation(activation)(batch1)\n",
        "    drop1 = Dropout(hidden_drop_rate)(act1)\n",
        "    # hidden layer 2\n",
        "    hidden2 = Dense(n2_units, kernel_initializer=init_mode, kernel_constraint=maxnorm(maxnorm_value))(drop1)\n",
        "    batch2 = BatchNormalization()(hidden2)\n",
        "    act2 = Activation(activation)(batch2)\n",
        "    drop2 = Dropout(hidden_drop_rate)(act2)\n",
        "    # hidden layer 3\n",
        "    hidden3 = Dense(n3_units, kernel_initializer=init_mode, kernel_constraint=maxnorm(maxnorm_value))(drop2)\n",
        "    batch3 = BatchNormalization()(hidden3)\n",
        "    act3 = Activation(activation)(batch3)\n",
        "    drop3 = Dropout(hidden_drop_rate)(act3)\n",
        "    # output layers\n",
        "    multi_output = [Dense(1, activation='sigmoid', kernel_initializer=init_mode)(drop3) for i in range(output_dim)]\n",
        "    model = Model(inputs=visible, outputs=multi_output)\n",
        "    # compile model\n",
        "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=[keras.metrics.AUC(name='roc_auc')])\n",
        "    return model\n",
        "\n",
        "\n",
        "# create custom sklearn classifier s.t. Keras functional API can be used in Pipeline and GridSearchCV\n",
        "class CustomClassifier(BaseEstimator, ClassifierMixin):\n",
        "\n",
        "    def __init__(self, n1_units=100, n2_units=100, n3_units=100, activation='sigmoid', \n",
        "                 optimizer='Adam', visible_drop_rate=0.2, hidden_drop_rate=0.5, \n",
        "                 init_mode='glorot_uniform', maxnorm_value=3,\n",
        "                 batch_size=None, epochs=1, class_weight=None):\n",
        "        self.n1_units = n1_units\n",
        "        self.n2_units = n2_units \n",
        "        self.n3_units = n3_units         \n",
        "        self.activation = activation\n",
        "        self.optimizer = optimizer\n",
        "        self.visible_drop_rate = visible_drop_rate\n",
        "        self.hidden_drop_rate = hidden_drop_rate\n",
        "        self.maxnorm_value = maxnorm_value\n",
        "        self.init_mode = init_mode\n",
        "        # model.fit parameters\n",
        "        self.batch_size = batch_size\n",
        "        self.epochs = epochs\n",
        "        self.class_weight = class_weight\n",
        "        \n",
        "\n",
        "    def fit(self, X, y, batch_size=None, epochs=1, verbose=0, \n",
        "            validation_data=None, class_weight=None):\n",
        "        # determine output dimension\n",
        "        if len(y.shape)==1:\n",
        "            self.output_dim_ = 1\n",
        "        else:\n",
        "            self.output_dim_ = y.shape[1]\n",
        "        \n",
        "        # reshape target into 2d-array\n",
        "        y_reshaped = y.reshape(y.shape[0], self.output_dim_)\n",
        "        \n",
        "        # create model\n",
        "        self.model_ = create_model(input_dim=X.shape[1], output_dim=self.output_dim_, \n",
        "                                   n1_units=self.n1_units, n2_units=self.n2_units, \n",
        "                                   n3_units=self.n3_units, activation=self.activation, \n",
        "                                   optimizer=self.optimizer, visible_drop_rate=self.visible_drop_rate, \n",
        "                                   hidden_drop_rate=self.hidden_drop_rate, init_mode=self.init_mode, \n",
        "                                   maxnorm_value=self.maxnorm_value)\n",
        "        \n",
        "        # fit parameters entered in fit method \n",
        "        # have priority over the same parameters entered in __init__\n",
        "        if batch_size is not None:\n",
        "            fit_batch_size = batch_size\n",
        "        else:\n",
        "            fit_batch_size = self.batch_size\n",
        "        if epochs is not 1:\n",
        "            fit_epochs = epochs\n",
        "        else: \n",
        "            fit_epochs = self.epochs\n",
        "        if class_weight is not None:\n",
        "            fit_class_weight = class_weight\n",
        "        else: \n",
        "            fit_class_weight = self.class_weight\n",
        "        \n",
        "        # fit model and save history in self.history_ attribute\n",
        "        self.history_ = self.model_.fit(\n",
        "            X, [y_reshaped[:, i] for i in range(self.output_dim_)], \n",
        "            batch_size=fit_batch_size, epochs=fit_epochs, verbose=verbose, \n",
        "            validation_data=validation_data, class_weight=fit_class_weight\n",
        "        )\n",
        "        # return classifier\n",
        "        return self\n",
        "    \n",
        "\n",
        "    def predict(self, X):\n",
        "        # make prediction\n",
        "        if self.output_dim_ == 1:\n",
        "            predictions = self.model_.predict(X, verbose=0)\n",
        "        else:\n",
        "            predictions = np.concatenate(self.model_.predict(X, verbose=0), axis=1)\n",
        "        return predictions\n",
        "    \n",
        "    \n",
        "    # same as predict but predict_proba needs to be defined in order to be able \n",
        "    # to use scoring='roc_auc' in GridSearchCV\n",
        "    def predict_proba(self, X):\n",
        "        return self.predict(X)\n",
        "\n",
        "\n",
        "# define pipeline\n",
        "steps = [('scaler', StandardScaler()), ('ANN', CustomClassifier())]\n",
        "pipeline = Pipeline(steps)\n",
        "\n",
        "# compute class_weight='balanced' as in sklearn.utils.class_weight.compute_class_weight\n",
        "def balanced_class_weight(col_name):\n",
        "    y = y_train[col_name]\n",
        "    class_weight = dict(len(y) / (y.nunique() * y.value_counts()))\n",
        "    return class_weight\n",
        "\n",
        "class_weight = [balanced_class_weight(col_name) for col_name in subtask_1+subtask_2]\n",
        "\n",
        "# define parameter choice for param_grid in GridSearchCV\n",
        "param_grid = [dict(ANN__n1_units=[600], ANN__n2_units=[3000], ANN__n3_units=[200], \n",
        "                  ANN__activation=['relu'], ANN__optimizer=['Adam'], \n",
        "                  ANN__visible_drop_rate=[0.3, 0.4], ANN__hidden_drop_rate=[0.4, 0.5], \n",
        "                  ANN__init_mode=['uniform'], ANN__maxnorm_value=[3], \n",
        "                  ANN__epochs=[15], ANN__batch_size=[256], ANN__class_weight=[class_weight]), \n",
        "              dict(ANN__n1_units=[800], ANN__n2_units=[1500], ANN__n3_units=[300], \n",
        "                  ANN__activation=['relu'], ANN__optimizer=['Adam'], \n",
        "                  ANN__visible_drop_rate=[0.3, 0.4], ANN__hidden_drop_rate=[0.4, 0.5], \n",
        "                  ANN__init_mode=['uniform'], ANN__maxnorm_value=[3], \n",
        "                  ANN__epochs=[15], ANN__batch_size=[256], ANN__class_weight=[class_weight]), \n",
        "              dict(ANN__n1_units=[1500], ANN__n2_units=[432], ANN__n3_units=[432], \n",
        "                  ANN__activation=['relu'], ANN__optimizer=['Adam'], \n",
        "                  ANN__visible_drop_rate=[0.3, 0.4], ANN__hidden_drop_rate=[0.4, 0.5], \n",
        "                  ANN__init_mode=['uniform'], ANN__maxnorm_value=[3], \n",
        "                  ANN__epochs=[15], ANN__batch_size=[256], ANN__class_weight=[class_weight])]\n",
        "                  \n",
        "\n",
        "# define classifier\n",
        "classifier = GridSearchCV(estimator=pipeline, param_grid=param_grid, n_jobs=-1, \n",
        "                          cv=5, verbose=50, scoring='roc_auc')\n",
        "\n",
        "print(\"# perform classifier grid search\")\n",
        "start_time = time.time()\n",
        "classifier.fit(X_train_clf, y_train[subtask_1+subtask_2].values)\n",
        "run_time = time.time()-start_time\n",
        "print('# perform classifier grid search: runtime: {a:.0f} h {b:.0f} min {c:.2f} s'.format(a=run_time//3600, b=(run_time-(run_time//3600)*3600)//60, c=run_time%60))\n",
        "\n",
        "print('# make predictions')\n",
        "df_predictions[subtask_1+subtask_2] = pd.DataFrame(classifier.predict(X_test_clf))\n",
        "\n",
        "print('# summarize GridSearchCV results')\n",
        "cv_results = pd.DataFrame(classifier.cv_results_).sort_values(by=['mean_test_score'], ascending=False).loc[:, ['mean_test_score', 'std_test_score', 'params']]\n",
        "for mean, stdev, param in zip(*[cv_results[col] for col in cv_results]):\n",
        "    print(\"{:f} ({:f}) with: {}\".format(mean, stdev, param))\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# perform classifier grid search\n",
            "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:   55.4s\n",
            "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:   55.6s\n",
            "[Parallel(n_jobs=-1)]: Done   3 tasks      | elapsed:  1.7min\n",
            "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:  1.7min\n",
            "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:  2.5min\n",
            "[Parallel(n_jobs=-1)]: Done   6 tasks      | elapsed:  2.6min\n",
            "[Parallel(n_jobs=-1)]: Done   7 tasks      | elapsed:  3.3min\n",
            "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:  3.3min\n",
            "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:  4.1min\n",
            "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:  4.1min\n",
            "[Parallel(n_jobs=-1)]: Done  11 tasks      | elapsed:  4.9min\n",
            "[Parallel(n_jobs=-1)]: Done  12 tasks      | elapsed:  4.9min\n",
            "[Parallel(n_jobs=-1)]: Done  13 tasks      | elapsed:  5.7min\n",
            "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed:  5.7min\n",
            "[Parallel(n_jobs=-1)]: Done  15 tasks      | elapsed:  6.4min\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:691: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:  6.5min\n",
            "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:  7.3min\n",
            "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:  7.3min\n",
            "[Parallel(n_jobs=-1)]: Done  19 tasks      | elapsed:  8.1min\n",
            "[Parallel(n_jobs=-1)]: Done  20 tasks      | elapsed:  8.1min\n",
            "[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:  8.9min\n",
            "[Parallel(n_jobs=-1)]: Done  22 tasks      | elapsed:  8.9min\n",
            "[Parallel(n_jobs=-1)]: Done  23 tasks      | elapsed:  9.6min\n",
            "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:  9.6min\n",
            "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed: 10.3min\n",
            "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed: 10.4min\n",
            "[Parallel(n_jobs=-1)]: Done  27 tasks      | elapsed: 11.1min\n",
            "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed: 11.1min\n",
            "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed: 11.9min\n",
            "[Parallel(n_jobs=-1)]: Done  30 tasks      | elapsed: 11.9min\n",
            "[Parallel(n_jobs=-1)]: Done  31 tasks      | elapsed: 12.6min\n",
            "[Parallel(n_jobs=-1)]: Done  32 tasks      | elapsed: 12.6min\n",
            "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed: 13.5min\n",
            "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed: 13.5min\n",
            "[Parallel(n_jobs=-1)]: Done  35 tasks      | elapsed: 14.2min\n",
            "[Parallel(n_jobs=-1)]: Done  36 tasks      | elapsed: 14.2min\n",
            "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed: 14.9min\n",
            "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed: 15.0min\n",
            "[Parallel(n_jobs=-1)]: Done  39 tasks      | elapsed: 15.7min\n",
            "[Parallel(n_jobs=-1)]: Done  40 tasks      | elapsed: 15.7min\n",
            "[Parallel(n_jobs=-1)]: Done  41 tasks      | elapsed: 16.4min\n",
            "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed: 16.5min\n",
            "[Parallel(n_jobs=-1)]: Done  43 tasks      | elapsed: 17.2min\n",
            "[Parallel(n_jobs=-1)]: Done  44 tasks      | elapsed: 17.2min\n",
            "[Parallel(n_jobs=-1)]: Done  45 tasks      | elapsed: 18.0min\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed: 18.0min\n",
            "[Parallel(n_jobs=-1)]: Done  47 tasks      | elapsed: 18.7min\n",
            "[Parallel(n_jobs=-1)]: Done  48 tasks      | elapsed: 18.7min\n",
            "[Parallel(n_jobs=-1)]: Done  49 tasks      | elapsed: 19.6min\n",
            "[Parallel(n_jobs=-1)]: Done  50 tasks      | elapsed: 19.6min\n",
            "[Parallel(n_jobs=-1)]: Done  51 tasks      | elapsed: 20.3min\n",
            "[Parallel(n_jobs=-1)]: Done  52 tasks      | elapsed: 20.3min\n",
            "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed: 21.1min\n",
            "[Parallel(n_jobs=-1)]: Done  54 tasks      | elapsed: 21.1min\n",
            "[Parallel(n_jobs=-1)]: Done  55 tasks      | elapsed: 21.9min\n",
            "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed: 21.9min\n",
            "[Parallel(n_jobs=-1)]: Done  57 tasks      | elapsed: 22.6min\n",
            "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed: 23.4min finished\n",
            "# perform classifier grid search: runtime: 0 h 24 min 4.44 s\n",
            "# make predictions\n",
            "# summarize GridSearchCV results\n",
            "0.817046 (0.004062) with: {'ANN__activation': 'relu', 'ANN__batch_size': 256, 'ANN__class_weight': [{0: 0.6833225411900137, 1: 1.8637166405023549}, {0: 0.5397840295538505, 1: 6.7839285714285715}, {0: 0.6576760612145973, 1: 2.0855292050944225}, {0: 0.6546388199614006, 1: 2.1166703811009584}, {0: 0.6584055459272097, 1: 2.0782275711159737}, {0: 0.6251645602948921, 1: 2.4973704969760715}, {0: 0.5554093567251462, 1: 5.011873350923483}, {0: 0.6524800769442154, 1: 2.1395584591124126}, {0: 0.517546727698763, 1: 14.747670807453416}, {0: 0.5353418634800744, 1: 7.573763955342903}, {0: 0.5303791813257386, 1: 8.729319852941176}], 'ANN__epochs': 15, 'ANN__hidden_drop_rate': 0.5, 'ANN__init_mode': 'uniform', 'ANN__maxnorm_value': 3, 'ANN__n1_units': 600, 'ANN__n2_units': 3000, 'ANN__n3_units': 200, 'ANN__optimizer': 'Adam', 'ANN__visible_drop_rate': 0.3}\n",
            "\n",
            "0.816881 (0.004829) with: {'ANN__activation': 'relu', 'ANN__batch_size': 256, 'ANN__class_weight': [{0: 0.6833225411900137, 1: 1.8637166405023549}, {0: 0.5397840295538505, 1: 6.7839285714285715}, {0: 0.6576760612145973, 1: 2.0855292050944225}, {0: 0.6546388199614006, 1: 2.1166703811009584}, {0: 0.6584055459272097, 1: 2.0782275711159737}, {0: 0.6251645602948921, 1: 2.4973704969760715}, {0: 0.5554093567251462, 1: 5.011873350923483}, {0: 0.6524800769442154, 1: 2.1395584591124126}, {0: 0.517546727698763, 1: 14.747670807453416}, {0: 0.5353418634800744, 1: 7.573763955342903}, {0: 0.5303791813257386, 1: 8.729319852941176}], 'ANN__epochs': 15, 'ANN__hidden_drop_rate': 0.5, 'ANN__init_mode': 'uniform', 'ANN__maxnorm_value': 3, 'ANN__n1_units': 1500, 'ANN__n2_units': 432, 'ANN__n3_units': 432, 'ANN__optimizer': 'Adam', 'ANN__visible_drop_rate': 0.3}\n",
            "\n",
            "0.816838 (0.004198) with: {'ANN__activation': 'relu', 'ANN__batch_size': 256, 'ANN__class_weight': [{0: 0.6833225411900137, 1: 1.8637166405023549}, {0: 0.5397840295538505, 1: 6.7839285714285715}, {0: 0.6576760612145973, 1: 2.0855292050944225}, {0: 0.6546388199614006, 1: 2.1166703811009584}, {0: 0.6584055459272097, 1: 2.0782275711159737}, {0: 0.6251645602948921, 1: 2.4973704969760715}, {0: 0.5554093567251462, 1: 5.011873350923483}, {0: 0.6524800769442154, 1: 2.1395584591124126}, {0: 0.517546727698763, 1: 14.747670807453416}, {0: 0.5353418634800744, 1: 7.573763955342903}, {0: 0.5303791813257386, 1: 8.729319852941176}], 'ANN__epochs': 15, 'ANN__hidden_drop_rate': 0.5, 'ANN__init_mode': 'uniform', 'ANN__maxnorm_value': 3, 'ANN__n1_units': 600, 'ANN__n2_units': 3000, 'ANN__n3_units': 200, 'ANN__optimizer': 'Adam', 'ANN__visible_drop_rate': 0.4}\n",
            "\n",
            "0.816637 (0.005093) with: {'ANN__activation': 'relu', 'ANN__batch_size': 256, 'ANN__class_weight': [{0: 0.6833225411900137, 1: 1.8637166405023549}, {0: 0.5397840295538505, 1: 6.7839285714285715}, {0: 0.6576760612145973, 1: 2.0855292050944225}, {0: 0.6546388199614006, 1: 2.1166703811009584}, {0: 0.6584055459272097, 1: 2.0782275711159737}, {0: 0.6251645602948921, 1: 2.4973704969760715}, {0: 0.5554093567251462, 1: 5.011873350923483}, {0: 0.6524800769442154, 1: 2.1395584591124126}, {0: 0.517546727698763, 1: 14.747670807453416}, {0: 0.5353418634800744, 1: 7.573763955342903}, {0: 0.5303791813257386, 1: 8.729319852941176}], 'ANN__epochs': 15, 'ANN__hidden_drop_rate': 0.5, 'ANN__init_mode': 'uniform', 'ANN__maxnorm_value': 3, 'ANN__n1_units': 800, 'ANN__n2_units': 1500, 'ANN__n3_units': 300, 'ANN__optimizer': 'Adam', 'ANN__visible_drop_rate': 0.3}\n",
            "\n",
            "0.816015 (0.003300) with: {'ANN__activation': 'relu', 'ANN__batch_size': 256, 'ANN__class_weight': [{0: 0.6833225411900137, 1: 1.8637166405023549}, {0: 0.5397840295538505, 1: 6.7839285714285715}, {0: 0.6576760612145973, 1: 2.0855292050944225}, {0: 0.6546388199614006, 1: 2.1166703811009584}, {0: 0.6584055459272097, 1: 2.0782275711159737}, {0: 0.6251645602948921, 1: 2.4973704969760715}, {0: 0.5554093567251462, 1: 5.011873350923483}, {0: 0.6524800769442154, 1: 2.1395584591124126}, {0: 0.517546727698763, 1: 14.747670807453416}, {0: 0.5353418634800744, 1: 7.573763955342903}, {0: 0.5303791813257386, 1: 8.729319852941176}], 'ANN__epochs': 15, 'ANN__hidden_drop_rate': 0.4, 'ANN__init_mode': 'uniform', 'ANN__maxnorm_value': 3, 'ANN__n1_units': 1500, 'ANN__n2_units': 432, 'ANN__n3_units': 432, 'ANN__optimizer': 'Adam', 'ANN__visible_drop_rate': 0.3}\n",
            "\n",
            "0.815986 (0.005435) with: {'ANN__activation': 'relu', 'ANN__batch_size': 256, 'ANN__class_weight': [{0: 0.6833225411900137, 1: 1.8637166405023549}, {0: 0.5397840295538505, 1: 6.7839285714285715}, {0: 0.6576760612145973, 1: 2.0855292050944225}, {0: 0.6546388199614006, 1: 2.1166703811009584}, {0: 0.6584055459272097, 1: 2.0782275711159737}, {0: 0.6251645602948921, 1: 2.4973704969760715}, {0: 0.5554093567251462, 1: 5.011873350923483}, {0: 0.6524800769442154, 1: 2.1395584591124126}, {0: 0.517546727698763, 1: 14.747670807453416}, {0: 0.5353418634800744, 1: 7.573763955342903}, {0: 0.5303791813257386, 1: 8.729319852941176}], 'ANN__epochs': 15, 'ANN__hidden_drop_rate': 0.4, 'ANN__init_mode': 'uniform', 'ANN__maxnorm_value': 3, 'ANN__n1_units': 1500, 'ANN__n2_units': 432, 'ANN__n3_units': 432, 'ANN__optimizer': 'Adam', 'ANN__visible_drop_rate': 0.4}\n",
            "\n",
            "0.815956 (0.004574) with: {'ANN__activation': 'relu', 'ANN__batch_size': 256, 'ANN__class_weight': [{0: 0.6833225411900137, 1: 1.8637166405023549}, {0: 0.5397840295538505, 1: 6.7839285714285715}, {0: 0.6576760612145973, 1: 2.0855292050944225}, {0: 0.6546388199614006, 1: 2.1166703811009584}, {0: 0.6584055459272097, 1: 2.0782275711159737}, {0: 0.6251645602948921, 1: 2.4973704969760715}, {0: 0.5554093567251462, 1: 5.011873350923483}, {0: 0.6524800769442154, 1: 2.1395584591124126}, {0: 0.517546727698763, 1: 14.747670807453416}, {0: 0.5353418634800744, 1: 7.573763955342903}, {0: 0.5303791813257386, 1: 8.729319852941176}], 'ANN__epochs': 15, 'ANN__hidden_drop_rate': 0.5, 'ANN__init_mode': 'uniform', 'ANN__maxnorm_value': 3, 'ANN__n1_units': 1500, 'ANN__n2_units': 432, 'ANN__n3_units': 432, 'ANN__optimizer': 'Adam', 'ANN__visible_drop_rate': 0.4}\n",
            "\n",
            "0.815930 (0.003477) with: {'ANN__activation': 'relu', 'ANN__batch_size': 256, 'ANN__class_weight': [{0: 0.6833225411900137, 1: 1.8637166405023549}, {0: 0.5397840295538505, 1: 6.7839285714285715}, {0: 0.6576760612145973, 1: 2.0855292050944225}, {0: 0.6546388199614006, 1: 2.1166703811009584}, {0: 0.6584055459272097, 1: 2.0782275711159737}, {0: 0.6251645602948921, 1: 2.4973704969760715}, {0: 0.5554093567251462, 1: 5.011873350923483}, {0: 0.6524800769442154, 1: 2.1395584591124126}, {0: 0.517546727698763, 1: 14.747670807453416}, {0: 0.5353418634800744, 1: 7.573763955342903}, {0: 0.5303791813257386, 1: 8.729319852941176}], 'ANN__epochs': 15, 'ANN__hidden_drop_rate': 0.4, 'ANN__init_mode': 'uniform', 'ANN__maxnorm_value': 3, 'ANN__n1_units': 600, 'ANN__n2_units': 3000, 'ANN__n3_units': 200, 'ANN__optimizer': 'Adam', 'ANN__visible_drop_rate': 0.3}\n",
            "\n",
            "0.815898 (0.002754) with: {'ANN__activation': 'relu', 'ANN__batch_size': 256, 'ANN__class_weight': [{0: 0.6833225411900137, 1: 1.8637166405023549}, {0: 0.5397840295538505, 1: 6.7839285714285715}, {0: 0.6576760612145973, 1: 2.0855292050944225}, {0: 0.6546388199614006, 1: 2.1166703811009584}, {0: 0.6584055459272097, 1: 2.0782275711159737}, {0: 0.6251645602948921, 1: 2.4973704969760715}, {0: 0.5554093567251462, 1: 5.011873350923483}, {0: 0.6524800769442154, 1: 2.1395584591124126}, {0: 0.517546727698763, 1: 14.747670807453416}, {0: 0.5353418634800744, 1: 7.573763955342903}, {0: 0.5303791813257386, 1: 8.729319852941176}], 'ANN__epochs': 15, 'ANN__hidden_drop_rate': 0.4, 'ANN__init_mode': 'uniform', 'ANN__maxnorm_value': 3, 'ANN__n1_units': 800, 'ANN__n2_units': 1500, 'ANN__n3_units': 300, 'ANN__optimizer': 'Adam', 'ANN__visible_drop_rate': 0.4}\n",
            "\n",
            "0.815804 (0.004866) with: {'ANN__activation': 'relu', 'ANN__batch_size': 256, 'ANN__class_weight': [{0: 0.6833225411900137, 1: 1.8637166405023549}, {0: 0.5397840295538505, 1: 6.7839285714285715}, {0: 0.6576760612145973, 1: 2.0855292050944225}, {0: 0.6546388199614006, 1: 2.1166703811009584}, {0: 0.6584055459272097, 1: 2.0782275711159737}, {0: 0.6251645602948921, 1: 2.4973704969760715}, {0: 0.5554093567251462, 1: 5.011873350923483}, {0: 0.6524800769442154, 1: 2.1395584591124126}, {0: 0.517546727698763, 1: 14.747670807453416}, {0: 0.5353418634800744, 1: 7.573763955342903}, {0: 0.5303791813257386, 1: 8.729319852941176}], 'ANN__epochs': 15, 'ANN__hidden_drop_rate': 0.4, 'ANN__init_mode': 'uniform', 'ANN__maxnorm_value': 3, 'ANN__n1_units': 600, 'ANN__n2_units': 3000, 'ANN__n3_units': 200, 'ANN__optimizer': 'Adam', 'ANN__visible_drop_rate': 0.4}\n",
            "\n",
            "0.815421 (0.003607) with: {'ANN__activation': 'relu', 'ANN__batch_size': 256, 'ANN__class_weight': [{0: 0.6833225411900137, 1: 1.8637166405023549}, {0: 0.5397840295538505, 1: 6.7839285714285715}, {0: 0.6576760612145973, 1: 2.0855292050944225}, {0: 0.6546388199614006, 1: 2.1166703811009584}, {0: 0.6584055459272097, 1: 2.0782275711159737}, {0: 0.6251645602948921, 1: 2.4973704969760715}, {0: 0.5554093567251462, 1: 5.011873350923483}, {0: 0.6524800769442154, 1: 2.1395584591124126}, {0: 0.517546727698763, 1: 14.747670807453416}, {0: 0.5353418634800744, 1: 7.573763955342903}, {0: 0.5303791813257386, 1: 8.729319852941176}], 'ANN__epochs': 15, 'ANN__hidden_drop_rate': 0.4, 'ANN__init_mode': 'uniform', 'ANN__maxnorm_value': 3, 'ANN__n1_units': 800, 'ANN__n2_units': 1500, 'ANN__n3_units': 300, 'ANN__optimizer': 'Adam', 'ANN__visible_drop_rate': 0.3}\n",
            "\n",
            "0.815203 (0.004517) with: {'ANN__activation': 'relu', 'ANN__batch_size': 256, 'ANN__class_weight': [{0: 0.6833225411900137, 1: 1.8637166405023549}, {0: 0.5397840295538505, 1: 6.7839285714285715}, {0: 0.6576760612145973, 1: 2.0855292050944225}, {0: 0.6546388199614006, 1: 2.1166703811009584}, {0: 0.6584055459272097, 1: 2.0782275711159737}, {0: 0.6251645602948921, 1: 2.4973704969760715}, {0: 0.5554093567251462, 1: 5.011873350923483}, {0: 0.6524800769442154, 1: 2.1395584591124126}, {0: 0.517546727698763, 1: 14.747670807453416}, {0: 0.5353418634800744, 1: 7.573763955342903}, {0: 0.5303791813257386, 1: 8.729319852941176}], 'ANN__epochs': 15, 'ANN__hidden_drop_rate': 0.5, 'ANN__init_mode': 'uniform', 'ANN__maxnorm_value': 3, 'ANN__n1_units': 800, 'ANN__n2_units': 1500, 'ANN__n3_units': 300, 'ANN__optimizer': 'Adam', 'ANN__visible_drop_rate': 0.4}\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJHart8IZEQX",
        "outputId": "621f11cc-23b8-44d1-bdf9-10ba0fe6d19d"
      },
      "source": [
        "#### REGRESSION: SUBTASK 3\n",
        "\n",
        "# prepare features\n",
        "relevant_features = ['Age', 'Temp', 'ABPd', 'ABPs', 'pH', 'Glucose', 'Hgb']+[col[len('LABEL_'):] for col in subtask_3]\n",
        "\n",
        "# same feature engineering as for classification above\n",
        "def regr_features(X):\n",
        "    X_mean = X[relevant_features].mean()\n",
        "\n",
        "    X_pid_mean = X.groupby(['pid'], as_index=False)[relevant_features].mean().drop(['pid'], axis=1)\n",
        "    X_pid_mean.fillna({col:X_mean[col] for col in X_pid_mean.columns}, inplace=True)\n",
        "\n",
        "    X_pid_min = X.groupby(['pid'], as_index=False)[relevant_features].min().drop(['pid', 'Age'], axis=1)\n",
        "    X_pid_min.fillna({col:X_mean[col] for col in X_pid_min.columns}, inplace=True)\n",
        "\n",
        "    X_pid_max = X.groupby(['pid'], as_index=False)[relevant_features].max().drop(['pid', 'Age'], axis=1)\n",
        "    X_pid_max.fillna({col:X_mean[col] for col in X_pid_max.columns}, inplace=True)\n",
        "\n",
        "    X_pid_diff_0 = X_pid_max-X_pid_min\n",
        "\n",
        "    X_pid_first = X.groupby(['pid'], as_index=False)[relevant_features].first().drop(['pid', 'Age'], axis=1)\n",
        "    X_pid_first.fillna({col:X_mean[col] for col in X_pid_first.columns}, inplace=True)\n",
        "\n",
        "    X_pid_last = X.groupby(['pid'], as_index=False)[relevant_features].last().drop(['pid', 'Age'], axis=1)\n",
        "    X_pid_last.fillna({col:X_mean[col] for col in X_pid_last.columns}, inplace=True)\n",
        "\n",
        "    X_pid_diff_1 = X_pid_last-X_pid_first\n",
        "\n",
        "    X_pid_missing = X.groupby(['pid'], as_index=False)[relevant_features].count().drop(['pid', 'Age'], axis=1)\n",
        "\n",
        "    X_regr = pd.concat([X_pid_mean, X_pid_min, X_pid_max, X_pid_diff_0, X_pid_first, \n",
        "                        X_pid_last, X_pid_diff_1, X_pid_missing], axis=1).values\n",
        "    return X_regr\n",
        "\n",
        "X_train_regr, X_test_regr = [regr_features(X) for X in [X_train, X_test]]\n",
        "\n",
        "# create regressor pipeline\n",
        "steps = [('scaler', StandardScaler()), ('regr', Ridge())]\n",
        "pipeline = Pipeline(steps)\n",
        "\n",
        "# define parameter choice\n",
        "# regr=[Ridge()]\n",
        "alpha = [10**i for i in range(-1, 4)]\n",
        "# regr=[RandomForestRegressor(n_jobs=-1)]\n",
        "max_features = ['sqrt']\n",
        "n_estimators = [300, 400, 600, 700]\n",
        "\n",
        "param_grid = [dict(regr=[Ridge()], \n",
        "                   regr__alpha=alpha), \n",
        "              dict(regr=[RandomForestRegressor()], \n",
        "                   regr__max_features=max_features, \n",
        "                   regr__n_estimators=n_estimators)]\n",
        "\n",
        "# create regressor\n",
        "regressor = GridSearchCV(pipeline, param_grid=param_grid, n_jobs=-1, cv=5, verbose=50, scoring='r2')\n",
        "\n",
        "print('# perform regression')\n",
        "start_time = time.time()\n",
        "best_scores = []\n",
        "for col_name in subtask_3:\n",
        "    regressor.fit(X_train_regr, y_train[col_name].values)\n",
        "    best_scores.append(regressor.best_score_)\n",
        "    df_predictions[col_name] = regressor.predict(X_test_regr)\n",
        "    print('########### {}: summarize GridSearchCV results'.format(col_name))\n",
        "    cv_results = pd.DataFrame(regressor.cv_results_).sort_values(by=['mean_test_score'], ascending=False).loc[:, ['mean_test_score', 'std_test_score', 'params']]\n",
        "    for mean, stdev, param in zip(*[cv_results[col] for col in cv_results]):\n",
        "        print(\"{:f} ({:f}) with:\".format(mean, stdev))\n",
        "        print(param)\n",
        "        print()\n",
        "\n",
        "print('########### Mean r2-score: {}'.format(np.mean(best_scores)))\n",
        "run_time = time.time()-start_time\n",
        "print('# perform regression: runtime: {a:.0f} h {b:.0f} min {c:.2f} s'.format(a=run_time//3600, b=(run_time-(run_time//3600)*3600)//60, c=run_time%60))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# perform regression\n",
            "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    0.2s\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1528s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.2s\n",
            "[Parallel(n_jobs=-1)]: Done   3 tasks      | elapsed:    0.2s\n",
            "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:    0.2s\n",
            "[Parallel(n_jobs=-1)]: Done   6 tasks      | elapsed:    0.4s\n",
            "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:    0.4s\n",
            "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    0.5s\n",
            "[Parallel(n_jobs=-1)]: Done  12 tasks      | elapsed:    0.6s\n",
            "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed:    0.7s\n",
            "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    0.7s\n",
            "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.8s\n",
            "[Parallel(n_jobs=-1)]: Done  20 tasks      | elapsed:    0.8s\n",
            "[Parallel(n_jobs=-1)]: Done  22 tasks      | elapsed:    1.0s\n",
            "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:    1.0s\n",
            "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   34.3s\n",
            "[Parallel(n_jobs=-1)]: Batch computation too slow (6.9225s.) Setting batch_size=1.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:691: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:  1.1min\n",
            "[Parallel(n_jobs=-1)]: Done  30 tasks      | elapsed:  1.7min\n",
            "[Parallel(n_jobs=-1)]: Done  31 tasks      | elapsed:  2.5min\n",
            "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  2.6min\n",
            "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  3.2min\n",
            "[Parallel(n_jobs=-1)]: Done  35 tasks      | elapsed:  3.4min\n",
            "[Parallel(n_jobs=-1)]: Done  36 tasks      | elapsed:  4.4min\n",
            "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:  4.5min\n",
            "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:  5.5min\n",
            "[Parallel(n_jobs=-1)]: Done  39 tasks      | elapsed:  5.6min\n",
            "[Parallel(n_jobs=-1)]: Done  40 tasks      | elapsed:  6.7min\n",
            "[Parallel(n_jobs=-1)]: Done  41 tasks      | elapsed:  6.9min\n",
            "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  8.0min\n",
            "[Parallel(n_jobs=-1)]: Done  43 out of  45 | elapsed:  8.3min remaining:   23.1s\n",
            "[Parallel(n_jobs=-1)]: Done  45 out of  45 | elapsed:  9.5min remaining:    0.0s\n",
            "[Parallel(n_jobs=-1)]: Done  45 out of  45 | elapsed:  9.5min finished\n",
            "########### LABEL_RRate: summarize GridSearchCV results\n",
            "0.437488 (0.016079) with:\n",
            "{'regr': RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
            "                      max_depth=None, max_features='sqrt', max_leaf_nodes=None,\n",
            "                      max_samples=None, min_impurity_decrease=0.0,\n",
            "                      min_impurity_split=None, min_samples_leaf=1,\n",
            "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
            "                      n_estimators=700, n_jobs=-1, oob_score=False,\n",
            "                      random_state=None, verbose=0, warm_start=False), 'regr__max_features': 'sqrt', 'regr__n_estimators': 700}\n",
            "\n",
            "0.436639 (0.016217) with:\n",
            "{'regr': RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
            "                      max_depth=None, max_features='sqrt', max_leaf_nodes=None,\n",
            "                      max_samples=None, min_impurity_decrease=0.0,\n",
            "                      min_impurity_split=None, min_samples_leaf=1,\n",
            "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
            "                      n_estimators=700, n_jobs=-1, oob_score=False,\n",
            "                      random_state=None, verbose=0, warm_start=False), 'regr__max_features': 'sqrt', 'regr__n_estimators': 600}\n",
            "\n",
            "0.436375 (0.015784) with:\n",
            "{'regr': RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
            "                      max_depth=None, max_features='sqrt', max_leaf_nodes=None,\n",
            "                      max_samples=None, min_impurity_decrease=0.0,\n",
            "                      min_impurity_split=None, min_samples_leaf=1,\n",
            "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
            "                      n_estimators=700, n_jobs=-1, oob_score=False,\n",
            "                      random_state=None, verbose=0, warm_start=False), 'regr__max_features': 'sqrt', 'regr__n_estimators': 300}\n",
            "\n",
            "0.436121 (0.016182) with:\n",
            "{'regr': RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
            "                      max_depth=None, max_features='sqrt', max_leaf_nodes=None,\n",
            "                      max_samples=None, min_impurity_decrease=0.0,\n",
            "                      min_impurity_split=None, min_samples_leaf=1,\n",
            "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
            "                      n_estimators=700, n_jobs=-1, oob_score=False,\n",
            "                      random_state=None, verbose=0, warm_start=False), 'regr__max_features': 'sqrt', 'regr__n_estimators': 400}\n",
            "\n",
            "0.423011 (0.011729) with:\n",
            "{'regr': Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
            "      normalize=False, random_state=None, solver='auto', tol=0.001), 'regr__alpha': 100}\n",
            "\n",
            "0.422713 (0.011535) with:\n",
            "{'regr': Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
            "      normalize=False, random_state=None, solver='auto', tol=0.001), 'regr__alpha': 10}\n",
            "\n",
            "0.422626 (0.011502) with:\n",
            "{'regr': Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
            "      normalize=False, random_state=None, solver='auto', tol=0.001), 'regr__alpha': 1}\n",
            "\n",
            "0.422614 (0.011498) with:\n",
            "{'regr': Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
            "      normalize=False, random_state=None, solver='auto', tol=0.001), 'regr__alpha': 0.1}\n",
            "\n",
            "0.420991 (0.012153) with:\n",
            "{'regr': Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
            "      normalize=False, random_state=None, solver='auto', tol=0.001), 'regr__alpha': 1000}\n",
            "\n",
            "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    0.8s\n",
            "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.8s\n",
            "[Parallel(n_jobs=-1)]: Done   3 tasks      | elapsed:    0.9s\n",
            "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:    0.9s\n",
            "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:    1.0s\n",
            "[Parallel(n_jobs=-1)]: Done   6 tasks      | elapsed:    1.0s\n",
            "[Parallel(n_jobs=-1)]: Done   7 tasks      | elapsed:    1.0s\n",
            "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:    1.1s\n",
            "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.1s\n",
            "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    1.1s\n",
            "[Parallel(n_jobs=-1)]: Done  11 tasks      | elapsed:    1.2s\n",
            "[Parallel(n_jobs=-1)]: Done  12 tasks      | elapsed:    1.2s\n",
            "[Parallel(n_jobs=-1)]: Done  13 tasks      | elapsed:    1.3s\n",
            "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed:    1.3s\n",
            "[Parallel(n_jobs=-1)]: Done  15 tasks      | elapsed:    1.4s\n",
            "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    1.4s\n",
            "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    1.5s\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1943s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    1.5s\n",
            "[Parallel(n_jobs=-1)]: Done  19 tasks      | elapsed:    1.5s\n",
            "[Parallel(n_jobs=-1)]: Done  20 tasks      | elapsed:    1.5s\n",
            "[Parallel(n_jobs=-1)]: Done  22 tasks      | elapsed:    1.7s\n",
            "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:    1.7s\n",
            "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   33.3s\n",
            "[Parallel(n_jobs=-1)]: Batch computation too slow (6.5529s.) Setting batch_size=1.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:691: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:  1.1min\n",
            "[Parallel(n_jobs=-1)]: Done  30 tasks      | elapsed:  1.6min\n",
            "[Parallel(n_jobs=-1)]: Done  31 tasks      | elapsed:  2.3min\n",
            "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  2.5min\n",
            "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  3.0min\n",
            "[Parallel(n_jobs=-1)]: Done  35 tasks      | elapsed:  3.2min\n",
            "[Parallel(n_jobs=-1)]: Done  36 tasks      | elapsed:  4.1min\n",
            "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:  4.3min\n",
            "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:  5.1min\n",
            "[Parallel(n_jobs=-1)]: Done  39 tasks      | elapsed:  5.3min\n",
            "[Parallel(n_jobs=-1)]: Done  40 tasks      | elapsed:  6.2min\n",
            "[Parallel(n_jobs=-1)]: Done  41 tasks      | elapsed:  6.6min\n",
            "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  7.4min\n",
            "[Parallel(n_jobs=-1)]: Done  43 out of  45 | elapsed:  7.8min remaining:   21.9s\n",
            "[Parallel(n_jobs=-1)]: Done  45 out of  45 | elapsed:  8.9min remaining:    0.0s\n",
            "[Parallel(n_jobs=-1)]: Done  45 out of  45 | elapsed:  8.9min finished\n",
            "########### LABEL_ABPm: summarize GridSearchCV results\n",
            "0.630273 (0.010676) with:\n",
            "{'regr': RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
            "                      max_depth=None, max_features='sqrt', max_leaf_nodes=None,\n",
            "                      max_samples=None, min_impurity_decrease=0.0,\n",
            "                      min_impurity_split=None, min_samples_leaf=1,\n",
            "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
            "                      n_estimators=600, n_jobs=-1, oob_score=False,\n",
            "                      random_state=None, verbose=0, warm_start=False), 'regr__max_features': 'sqrt', 'regr__n_estimators': 600}\n",
            "\n",
            "0.629885 (0.010997) with:\n",
            "{'regr': RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
            "                      max_depth=None, max_features='sqrt', max_leaf_nodes=None,\n",
            "                      max_samples=None, min_impurity_decrease=0.0,\n",
            "                      min_impurity_split=None, min_samples_leaf=1,\n",
            "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
            "                      n_estimators=600, n_jobs=-1, oob_score=False,\n",
            "                      random_state=None, verbose=0, warm_start=False), 'regr__max_features': 'sqrt', 'regr__n_estimators': 700}\n",
            "\n",
            "0.629452 (0.011255) with:\n",
            "{'regr': RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
            "                      max_depth=None, max_features='sqrt', max_leaf_nodes=None,\n",
            "                      max_samples=None, min_impurity_decrease=0.0,\n",
            "                      min_impurity_split=None, min_samples_leaf=1,\n",
            "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
            "                      n_estimators=600, n_jobs=-1, oob_score=False,\n",
            "                      random_state=None, verbose=0, warm_start=False), 'regr__max_features': 'sqrt', 'regr__n_estimators': 400}\n",
            "\n",
            "0.629179 (0.011453) with:\n",
            "{'regr': RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
            "                      max_depth=None, max_features='sqrt', max_leaf_nodes=None,\n",
            "                      max_samples=None, min_impurity_decrease=0.0,\n",
            "                      min_impurity_split=None, min_samples_leaf=1,\n",
            "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
            "                      n_estimators=600, n_jobs=-1, oob_score=False,\n",
            "                      random_state=None, verbose=0, warm_start=False), 'regr__max_features': 'sqrt', 'regr__n_estimators': 300}\n",
            "\n",
            "0.623419 (0.012779) with:\n",
            "{'regr': Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
            "      normalize=False, random_state=None, solver='auto', tol=0.001), 'regr__alpha': 10}\n",
            "\n",
            "0.623386 (0.012851) with:\n",
            "{'regr': Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
            "      normalize=False, random_state=None, solver='auto', tol=0.001), 'regr__alpha': 1}\n",
            "\n",
            "0.623381 (0.012859) with:\n",
            "{'regr': Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
            "      normalize=False, random_state=None, solver='auto', tol=0.001), 'regr__alpha': 0.1}\n",
            "\n",
            "0.623020 (0.012273) with:\n",
            "{'regr': Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
            "      normalize=False, random_state=None, solver='auto', tol=0.001), 'regr__alpha': 100}\n",
            "\n",
            "0.617996 (0.011377) with:\n",
            "{'regr': Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
            "      normalize=False, random_state=None, solver='auto', tol=0.001), 'regr__alpha': 1000}\n",
            "\n",
            "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1291s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.2s\n",
            "[Parallel(n_jobs=-1)]: Done   3 tasks      | elapsed:    0.3s\n",
            "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:    0.4s\n",
            "[Parallel(n_jobs=-1)]: Done   6 tasks      | elapsed:    0.5s\n",
            "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:    0.7s\n",
            "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    0.8s\n",
            "[Parallel(n_jobs=-1)]: Done  12 tasks      | elapsed:    0.9s\n",
            "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed:    1.0s\n",
            "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    1.0s\n",
            "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    1.1s\n",
            "[Parallel(n_jobs=-1)]: Done  20 tasks      | elapsed:    1.2s\n",
            "[Parallel(n_jobs=-1)]: Done  22 tasks      | elapsed:    1.3s\n",
            "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:    1.3s\n",
            "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   36.4s\n",
            "[Parallel(n_jobs=-1)]: Batch computation too slow (7.3410s.) Setting batch_size=1.\n",
            "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:  1.2min\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:691: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done  30 tasks      | elapsed:  1.8min\n",
            "[Parallel(n_jobs=-1)]: Done  31 tasks      | elapsed:  2.6min\n",
            "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  2.7min\n",
            "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  3.3min\n",
            "[Parallel(n_jobs=-1)]: Done  35 tasks      | elapsed:  3.5min\n",
            "[Parallel(n_jobs=-1)]: Done  36 tasks      | elapsed:  4.5min\n",
            "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:  4.6min\n",
            "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:  5.7min\n",
            "[Parallel(n_jobs=-1)]: Done  39 tasks      | elapsed:  5.8min\n",
            "[Parallel(n_jobs=-1)]: Done  40 tasks      | elapsed:  6.8min\n",
            "[Parallel(n_jobs=-1)]: Done  41 tasks      | elapsed:  7.1min\n",
            "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  8.2min\n",
            "[Parallel(n_jobs=-1)]: Done  43 out of  45 | elapsed:  8.5min remaining:   23.7s\n",
            "[Parallel(n_jobs=-1)]: Done  45 out of  45 | elapsed:  9.7min remaining:    0.0s\n",
            "[Parallel(n_jobs=-1)]: Done  45 out of  45 | elapsed:  9.7min finished\n",
            "########### LABEL_SpO2: summarize GridSearchCV results\n",
            "0.418143 (0.054102) with:\n",
            "{'regr': Ridge(alpha=100, copy_X=True, fit_intercept=True, max_iter=None,\n",
            "      normalize=False, random_state=None, solver='auto', tol=0.001), 'regr__alpha': 100}\n",
            "\n",
            "0.417990 (0.054240) with:\n",
            "{'regr': Ridge(alpha=100, copy_X=True, fit_intercept=True, max_iter=None,\n",
            "      normalize=False, random_state=None, solver='auto', tol=0.001), 'regr__alpha': 10}\n",
            "\n",
            "0.417935 (0.054293) with:\n",
            "{'regr': Ridge(alpha=100, copy_X=True, fit_intercept=True, max_iter=None,\n",
            "      normalize=False, random_state=None, solver='auto', tol=0.001), 'regr__alpha': 1}\n",
            "\n",
            "0.417926 (0.054301) with:\n",
            "{'regr': Ridge(alpha=100, copy_X=True, fit_intercept=True, max_iter=None,\n",
            "      normalize=False, random_state=None, solver='auto', tol=0.001), 'regr__alpha': 0.1}\n",
            "\n",
            "0.415189 (0.053578) with:\n",
            "{'regr': Ridge(alpha=100, copy_X=True, fit_intercept=True, max_iter=None,\n",
            "      normalize=False, random_state=None, solver='auto', tol=0.001), 'regr__alpha': 1000}\n",
            "\n",
            "0.403631 (0.025543) with:\n",
            "{'regr': RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
            "                      max_depth=None, max_features='sqrt', max_leaf_nodes=None,\n",
            "                      max_samples=None, min_impurity_decrease=0.0,\n",
            "                      min_impurity_split=None, min_samples_leaf=1,\n",
            "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
            "                      n_estimators=600, n_jobs=-1, oob_score=False,\n",
            "                      random_state=None, verbose=0, warm_start=False), 'regr__max_features': 'sqrt', 'regr__n_estimators': 700}\n",
            "\n",
            "0.402702 (0.023372) with:\n",
            "{'regr': RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
            "                      max_depth=None, max_features='sqrt', max_leaf_nodes=None,\n",
            "                      max_samples=None, min_impurity_decrease=0.0,\n",
            "                      min_impurity_split=None, min_samples_leaf=1,\n",
            "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
            "                      n_estimators=600, n_jobs=-1, oob_score=False,\n",
            "                      random_state=None, verbose=0, warm_start=False), 'regr__max_features': 'sqrt', 'regr__n_estimators': 400}\n",
            "\n",
            "0.401053 (0.024944) with:\n",
            "{'regr': RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
            "                      max_depth=None, max_features='sqrt', max_leaf_nodes=None,\n",
            "                      max_samples=None, min_impurity_decrease=0.0,\n",
            "                      min_impurity_split=None, min_samples_leaf=1,\n",
            "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
            "                      n_estimators=600, n_jobs=-1, oob_score=False,\n",
            "                      random_state=None, verbose=0, warm_start=False), 'regr__max_features': 'sqrt', 'regr__n_estimators': 600}\n",
            "\n",
            "0.399784 (0.022204) with:\n",
            "{'regr': RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
            "                      max_depth=None, max_features='sqrt', max_leaf_nodes=None,\n",
            "                      max_samples=None, min_impurity_decrease=0.0,\n",
            "                      min_impurity_split=None, min_samples_leaf=1,\n",
            "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
            "                      n_estimators=600, n_jobs=-1, oob_score=False,\n",
            "                      random_state=None, verbose=0, warm_start=False), 'regr__max_features': 'sqrt', 'regr__n_estimators': 300}\n",
            "\n",
            "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:691: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    1.0s\n",
            "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    1.0s\n",
            "[Parallel(n_jobs=-1)]: Done   3 tasks      | elapsed:    1.0s\n",
            "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:    1.1s\n",
            "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:    1.1s\n",
            "[Parallel(n_jobs=-1)]: Done   6 tasks      | elapsed:    1.2s\n",
            "[Parallel(n_jobs=-1)]: Done   7 tasks      | elapsed:    1.3s\n",
            "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:    1.4s\n",
            "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.4s\n",
            "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    1.5s\n",
            "[Parallel(n_jobs=-1)]: Done  11 tasks      | elapsed:    1.6s\n",
            "[Parallel(n_jobs=-1)]: Done  12 tasks      | elapsed:    1.6s\n",
            "[Parallel(n_jobs=-1)]: Done  13 tasks      | elapsed:    1.7s\n",
            "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed:    1.8s\n",
            "[Parallel(n_jobs=-1)]: Done  15 tasks      | elapsed:    1.8s\n",
            "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    1.9s\n",
            "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    1.9s\n",
            "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    2.0s\n",
            "[Parallel(n_jobs=-1)]: Done  19 tasks      | elapsed:    2.0s\n",
            "[Parallel(n_jobs=-1)]: Done  20 tasks      | elapsed:    2.0s\n",
            "[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:    2.0s\n",
            "[Parallel(n_jobs=-1)]: Done  22 tasks      | elapsed:    2.1s\n",
            "[Parallel(n_jobs=-1)]: Done  23 tasks      | elapsed:    2.1s\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1997s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:    2.2s\n",
            "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    2.2s\n",
            "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   33.8s\n",
            "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:  1.1min\n",
            "[Parallel(n_jobs=-1)]: Batch computation too slow (62.8287s.) Setting batch_size=1.\n",
            "[Parallel(n_jobs=-1)]: Done  30 tasks      | elapsed:  1.6min\n",
            "[Parallel(n_jobs=-1)]: Done  32 tasks      | elapsed:  2.5min\n",
            "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  3.0min\n",
            "[Parallel(n_jobs=-1)]: Done  35 tasks      | elapsed:  3.2min\n",
            "[Parallel(n_jobs=-1)]: Done  36 tasks      | elapsed:  4.1min\n",
            "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:  4.3min\n",
            "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:  5.2min\n",
            "[Parallel(n_jobs=-1)]: Done  39 tasks      | elapsed:  5.3min\n",
            "[Parallel(n_jobs=-1)]: Done  40 tasks      | elapsed:  6.3min\n",
            "[Parallel(n_jobs=-1)]: Done  41 tasks      | elapsed:  6.6min\n",
            "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  7.5min\n",
            "[Parallel(n_jobs=-1)]: Done  43 out of  45 | elapsed:  7.9min remaining:   22.0s\n",
            "[Parallel(n_jobs=-1)]: Done  45 out of  45 | elapsed:  9.0min remaining:    0.0s\n",
            "[Parallel(n_jobs=-1)]: Done  45 out of  45 | elapsed:  9.0min finished\n",
            "########### LABEL_Heartrate: summarize GridSearchCV results\n",
            "0.669198 (0.008324) with:\n",
            "{'regr': RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
            "                      max_depth=None, max_features='sqrt', max_leaf_nodes=None,\n",
            "                      max_samples=None, min_impurity_decrease=0.0,\n",
            "                      min_impurity_split=None, min_samples_leaf=1,\n",
            "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
            "                      n_estimators=700, n_jobs=-1, oob_score=False,\n",
            "                      random_state=None, verbose=0, warm_start=False), 'regr__max_features': 'sqrt', 'regr__n_estimators': 700}\n",
            "\n",
            "0.668437 (0.008392) with:\n",
            "{'regr': RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
            "                      max_depth=None, max_features='sqrt', max_leaf_nodes=None,\n",
            "                      max_samples=None, min_impurity_decrease=0.0,\n",
            "                      min_impurity_split=None, min_samples_leaf=1,\n",
            "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
            "                      n_estimators=700, n_jobs=-1, oob_score=False,\n",
            "                      random_state=None, verbose=0, warm_start=False), 'regr__max_features': 'sqrt', 'regr__n_estimators': 600}\n",
            "\n",
            "0.668397 (0.008052) with:\n",
            "{'regr': RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
            "                      max_depth=None, max_features='sqrt', max_leaf_nodes=None,\n",
            "                      max_samples=None, min_impurity_decrease=0.0,\n",
            "                      min_impurity_split=None, min_samples_leaf=1,\n",
            "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
            "                      n_estimators=700, n_jobs=-1, oob_score=False,\n",
            "                      random_state=None, verbose=0, warm_start=False), 'regr__max_features': 'sqrt', 'regr__n_estimators': 400}\n",
            "\n",
            "0.668034 (0.008407) with:\n",
            "{'regr': RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
            "                      max_depth=None, max_features='sqrt', max_leaf_nodes=None,\n",
            "                      max_samples=None, min_impurity_decrease=0.0,\n",
            "                      min_impurity_split=None, min_samples_leaf=1,\n",
            "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
            "                      n_estimators=700, n_jobs=-1, oob_score=False,\n",
            "                      random_state=None, verbose=0, warm_start=False), 'regr__max_features': 'sqrt', 'regr__n_estimators': 300}\n",
            "\n",
            "0.665218 (0.010294) with:\n",
            "{'regr': Ridge(alpha=100, copy_X=True, fit_intercept=True, max_iter=None,\n",
            "      normalize=False, random_state=None, solver='auto', tol=0.001), 'regr__alpha': 100}\n",
            "\n",
            "0.665167 (0.010115) with:\n",
            "{'regr': Ridge(alpha=100, copy_X=True, fit_intercept=True, max_iter=None,\n",
            "      normalize=False, random_state=None, solver='auto', tol=0.001), 'regr__alpha': 10}\n",
            "\n",
            "0.665135 (0.010115) with:\n",
            "{'regr': Ridge(alpha=100, copy_X=True, fit_intercept=True, max_iter=None,\n",
            "      normalize=False, random_state=None, solver='auto', tol=0.001), 'regr__alpha': 1}\n",
            "\n",
            "0.665130 (0.010118) with:\n",
            "{'regr': Ridge(alpha=100, copy_X=True, fit_intercept=True, max_iter=None,\n",
            "      normalize=False, random_state=None, solver='auto', tol=0.001), 'regr__alpha': 0.1}\n",
            "\n",
            "0.664259 (0.010611) with:\n",
            "{'regr': Ridge(alpha=100, copy_X=True, fit_intercept=True, max_iter=None,\n",
            "      normalize=False, random_state=None, solver='auto', tol=0.001), 'regr__alpha': 1000}\n",
            "\n",
            "########### Mean r2-score: 0.5387756951419852\n",
            "# perform regression: runtime: 0 h 39 min 33.91 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGKkAbDuZEQY"
      },
      "source": [
        "# df_predictions is a pandas dataframe containing the final result\n",
        "filename = 'Submission_{}.zip'.format(main_filename)\n",
        "df_predictions[['pid']+subtask_1+subtask_2+subtask_3].to_csv(filename, index=False, float_format='%.5f', compression='zip')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}